{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b00b9b-53c8-4015-9b61-b735ef6095e9",
   "metadata": {},
   "source": [
    "# RAG System Component Testing Notebook\n",
    "\n",
    "## Systematic validation of all modules in the Universal Knowledge Ingestion System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52fcc5-b374-4cab-8e4a-7386fbf667ee",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54dbee4-2ca4-45ed-9cb8-b64cd8be3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 16:18:14 - \u001b[34mroot\u001b[0m - \u001b[32mINFO\u001b[0m - Logging configured: level=DEBUG, console=True, file=True\n",
      "================================================================================\n",
      "RAG PIPELINE - INTEGRATION TESTING\n",
      "Started: 2025-12-03 16:18:18\n",
      "\n",
      "\n",
      "ðŸ“ Checking test files...\n",
      "âœ“ SCANNED_PDF: test_scanned.pdf (5.16 MB)\n",
      "\n",
      "âœ“ Found 1 test files\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from config.settings import settings\n",
    "from config.settings import get_settings\n",
    "from utils.text_cleaner import TextCleaner\n",
    "from config.logging_config import get_logger\n",
    "from vector_store.bm25_index import BM25Index\n",
    "from chunking.token_counter import TokenCounter\n",
    "from chunking.fixed_chunker import FixedChunker\n",
    "from embeddings.bge_embedder import get_embedder\n",
    "from vector_store.bm25_index import get_bm25_index\n",
    "from vector_store.index_builder import IndexBuilder\n",
    "from chunking.overlap_manager import OverlapManager\n",
    "from vector_store.faiss_manager import FAISSManager\n",
    "from embeddings.model_loader import get_model_loader\n",
    "from chunking.semantic_chunker import SemanticChunker\n",
    "from vector_store.backup_manager import BackupManager\n",
    "from vector_store.metadata_store import MetadataStore\n",
    "from vector_store.index_persister import IndexPersister\n",
    "from vector_store.faiss_manager import get_faiss_manager\n",
    "from vector_store.index_builder import get_index_builder\n",
    "from embeddings.embedding_cache import get_embedding_cache\n",
    "from embeddings.batch_processor import get_batch_processor\n",
    "from vector_store.backup_manager import get_backup_manager\n",
    "from vector_store.metadata_store import get_metadata_store\n",
    "from vector_store.index_persister import get_index_persister\n",
    "from chunking.hierarchical_chunker import HierarchicalChunker\n",
    "from document_parser.parser_factory import get_parser_factory\n",
    "from chunking.adaptive_selector import AdaptiveChunkingSelector\n",
    "\n",
    "\n",
    "# Setup\n",
    "print(\"=\" * 80)\n",
    "print(\"RAG PIPELINE - INTEGRATION TESTING\")\n",
    "\n",
    "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Test files Paths\n",
    "TEST_FILES = {#\"pdf\"         : Path(\"test.pdf\"),\n",
    "              \"scanned_pdf\" : Path(\"test_scanned.pdf\"),\n",
    "              #\"docx\"        : Path(\"test.docx\"),\n",
    "              #\"txt\"         : Path(\"test.txt\"),\n",
    "              #\"zip\"         : Path(\"test.zip\"),\n",
    "             }\n",
    "\n",
    "print(\"\\nðŸ“ Checking test files...\")\n",
    "\n",
    "available_files = dict()\n",
    "\n",
    "for file_type, file_path in TEST_FILES.items():\n",
    "    if file_path.exists():\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"âœ“ {file_type.upper()}: {file_path.name} ({size_mb:.2f} MB)\")\n",
    "        available_files[file_type] = file_path\n",
    "    \n",
    "    else:\n",
    "        print(f\"âœ— {file_type.upper()}: Not found at {file_path}\")\n",
    "\n",
    "if not available_files:\n",
    "    print(\"\\nâš ï¸  No test files found! Please update TEST_FILES paths.\")\n",
    "    print(\"Example:\")\n",
    "    print('  TEST_FILES = {')\n",
    "    print('      \"pdf\": Path(\"data/test.pdf\"),')\n",
    "    print('      \"txt\": Path(\"data/test.txt\"),')\n",
    "    print('  }')\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nâœ“ Found {len(available_files)} test files\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd74b4-642b-4f4e-8678-63397985ca0d",
   "metadata": {},
   "source": [
    "### DOCUMENT PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbad778-3d09-4819-9ab8-01dc0a26d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOCUMENT PARSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "parser_factory = get_parser_factory()\n",
    "\n",
    "print(f\"\\nðŸ“‹ Supported extensions: {', '.join(parser_factory.get_supported_extensions())}\")\n",
    "\n",
    "parsed_documents = dict()\n",
    "\n",
    "for file_type, file_path in available_files.items():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Parsing: {file_type.upper()} - {file_path.name}\")\n",
    "    print('=' * 60)\n",
    "    \n",
    "    try:\n",
    "        # Parse document\n",
    "        text, metadata              = parser_factory.parse(file_path        = file_path,\n",
    "                                                           extract_metadata = True,\n",
    "                                                           clean_text       = True,\n",
    "                                                          )\n",
    "        \n",
    "        # Store results\n",
    "        parsed_documents[file_type] = {'text'      : text,\n",
    "                                       'metadata'  : metadata,\n",
    "                                       'file_path' : file_path,\n",
    "                                      }\n",
    "\n",
    "        # Display info\n",
    "        print(f\"âœ“ Parsed successfully!\")\n",
    "        print(f\"  Document ID: {metadata.document_id}\")\n",
    "        print(f\"  Type: {metadata.document_type.value}\")\n",
    "        print(f\"  Text Length: {len(text):,} characters\")\n",
    "        print(f\"  File Size: {metadata.file_size_mb:.2f} MB\")\n",
    "        \n",
    "        if metadata.num_pages:\n",
    "            print(f\"  Pages: {metadata.num_pages}\")\n",
    "        \n",
    "        if metadata.title:\n",
    "            print(f\"  Title: {metadata.title}\")\n",
    "        \n",
    "        # Show sample text\n",
    "        print(f\"\\nðŸ“„ Sample text (first 200 chars):\")\n",
    "        print(f\"  {text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to parse: {e}\")\n",
    "        logger.error(f\"Parse error for {file_type}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ“ Successfully parsed {len(parsed_documents)}/{len(available_files)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f580ed-8663-45ba-b75a-623941c40f64",
   "metadata": {},
   "source": [
    "### TEXT CLEANING VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b175d4a-0770-4158-b6db-e5c4b8024887",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEXT CLEANING VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if parsed_documents:\n",
    "    # Take first document as example\n",
    "    sample_type        = list(parsed_documents.keys())[0]\n",
    "    sample_text        = parsed_documents[sample_type]['text']\n",
    "    \n",
    "    print(f\"\\nTesting text cleaning on {sample_type.upper()}...\")\n",
    "    \n",
    "    # Original stats\n",
    "    original_length    = len(sample_text)\n",
    "    original_sentences = len(TextCleaner.extract_sentences(sample_text))\n",
    "    \n",
    "    print(f\"  Original: {original_length:,} chars, {original_sentences} sentences\")\n",
    "    \n",
    "    # Clean text\n",
    "    cleaned_text       = TextCleaner.clean(text                 = sample_text,\n",
    "                                           remove_html          = True,\n",
    "                                           normalize_whitespace = True,\n",
    "                                           preserve_structure   = True,\n",
    "                                          )\n",
    "                                        \n",
    "    cleaned_length     = len(cleaned_text)\n",
    "    cleaned_sentences  = len(TextCleaner.extract_sentences(cleaned_text))\n",
    "    reduction          = ((original_length - cleaned_length) / original_length * 100) if (original_length > 0) else 0\n",
    "    \n",
    "    print(f\"  Cleaned: {cleaned_length:,} chars, {cleaned_sentences} sentences\")\n",
    "    print(f\"  Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nðŸ“„ Cleaned sample:\")\n",
    "    print(f\"  {cleaned_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a748b0-7056-46d3-8c1e-a8d8984e56db",
   "metadata": {},
   "source": [
    "### TOKEN COUNTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4938561-7bcb-4571-b6fe-ccdedc391d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOKEN COUNTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "token_counter = TokenCounter(tokenizer_type = \"cl100k_base\")\n",
    "\n",
    "print(f\"Tokenizer: {token_counter.tokenizer_type}\")\n",
    "\n",
    "token_stats = dict()\n",
    "\n",
    "for file_type, doc_data in parsed_documents.items():\n",
    "    text                   = doc_data['text']\n",
    "    stats                  = token_counter.get_token_stats(text = text)\n",
    "    token_stats[file_type] = stats\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {file_type.upper()}:\")\n",
    "    print(f\"  Tokens: {stats['tokens']:,}\")\n",
    "    print(f\"  Characters: {stats['characters']:,}\")\n",
    "    print(f\"  Words: {stats['words']:,}\")\n",
    "    print(f\"  Chars/Token: {stats['chars_per_token']:.2f}\")\n",
    "    print(f\"  Tokens/Word: {stats['tokens_per_word']:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cc059-278b-4e38-971a-fdf0c23bd8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_doc = parsed_documents['scanned_pdf']\n",
    "ocr_doc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3381c4-b4b1-445c-aa0f-cd29f6d1271c",
   "metadata": {},
   "source": [
    "### CHUNKING STRATEGIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3640e33d-92e3-430c-83d1-48f3201f2344",
   "metadata": {},
   "source": [
    "#### Test 1: Fixed Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ead1a-e731-4727-b7a8-cd7bec5e5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first available document\n",
    "if parsed_documents:\n",
    "    test_type     = list(parsed_documents.keys())[1]\n",
    "    test_data     = parsed_documents[test_type]\n",
    "    test_text     = test_data['text']\n",
    "    test_metadata = test_data['metadata']\n",
    "    \n",
    "    print(f\"\\nðŸ”ª Testing chunking on: {test_type.upper()}\")\n",
    "    print(f\"   Document: {test_metadata.filename}\")\n",
    "    print(f\"   Tokens: {token_stats[test_type]['tokens']:,}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"TEST 1: Fixed Chunking\")\n",
    "    print('=' * 60)\n",
    "    \n",
    "    fixed_chunker = FixedChunker(chunk_size                  = 512,\n",
    "                                 overlap                     = 50,\n",
    "                                 respect_sentence_boundaries = True\n",
    "                                )\n",
    "    \n",
    "    fixed_chunks  = fixed_chunker.chunk_text(text     = test_text, \n",
    "                                             metadata = test_metadata,\n",
    "                                            )\n",
    "    \n",
    "    fixed_stats   = fixed_chunker.get_chunk_statistics(chunks = fixed_chunks)\n",
    "    \n",
    "    print(f\"âœ“ Created {len(fixed_chunks)} chunks\")\n",
    "    print(f\"  Total Tokens: {fixed_stats['total_tokens']:,}\")\n",
    "    print(f\"  Avg Tokens/Chunk: {fixed_stats['avg_tokens_per_chunk']:.1f}\")\n",
    "    print(f\"  Min Tokens: {fixed_stats['min_tokens']}\")\n",
    "    print(f\"  Max Tokens: {fixed_stats['max_tokens']}\")\n",
    "    \n",
    "    # Show first chunk\n",
    "    if fixed_chunks:\n",
    "        for i, chunk in enumerate(fixed_chunks):\n",
    "            print(f\"\\nðŸ“„ Chunk sample: {i}\")\n",
    "            print(f\"  ID: {chunk.chunk_id}\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\")\n",
    "            print(f\"  Text: {chunk.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4145a-4b89-4882-9537-2d087d7f18fd",
   "metadata": {},
   "source": [
    "#### Test 2: Semantic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6ed05-fb63-4b28-bc4a-f17fda09f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first available document\n",
    "if parsed_documents:\n",
    "    test_type     = list(parsed_documents.keys())[0]\n",
    "    test_data     = parsed_documents[test_type]\n",
    "    test_text     = test_data['text']\n",
    "    test_metadata = test_data['metadata']\n",
    "    \n",
    "    print(f\"\\nðŸ”ª Testing chunking on: {test_type.upper()}\")\n",
    "    print(f\"   Document: {test_metadata.filename}\")\n",
    "    print(f\"   Tokens: {token_stats[test_type]['tokens']:,}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"TEST 2: Semantic Chunking\")\n",
    "    print('=' * 60)\n",
    "    \n",
    "    try:\n",
    "        semantic_chunker = SemanticChunker(chunk_size           = 512,\n",
    "                                           overlap              = 50,\n",
    "                                           similarity_threshold = 0.95,\n",
    "                                          )\n",
    "        \n",
    "        semantic_chunks  = semantic_chunker.chunk_text(text     = test_text, \n",
    "                                                       metadata = test_metadata,\n",
    "                                                      )\n",
    "        \n",
    "        semantic_stats   = semantic_chunker.get_chunk_statistics(chunks = semantic_chunks)\n",
    "        \n",
    "        print(f\"âœ“ Created {len(semantic_chunks)} semantic chunks\")\n",
    "        print(f\"  Total Tokens: {semantic_stats['total_tokens']:,}\")\n",
    "        print(f\"  Avg Tokens/Chunk: {semantic_stats['avg_tokens_per_chunk']:.1f}\")\n",
    "        \n",
    "        if semantic_chunks:\n",
    "            for i, chunk in enumerate(semantic_chunks):\n",
    "                print(f\"\\nðŸ“„ Semantic chunk: {i}\")\n",
    "                print(f\"  Tokens: {chunk.token_count}\")\n",
    "                print(f\"  Text: {chunk.text}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Semantic chunking unavailable: {e}\")\n",
    "        print(\"   (Embedding model may need to be downloaded)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c046192-0f92-4a29-981a-de2d7ebd0863",
   "metadata": {},
   "source": [
    "#### Test 3: Hierarchical Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18a511-84f5-4306-a429-5c3abdd4df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first available document\n",
    "if parsed_documents:\n",
    "    test_type     = list(parsed_documents.keys())[0]\n",
    "    test_data     = parsed_documents[test_type]\n",
    "    test_text     = test_data['text']\n",
    "    test_metadata = test_data['metadata']\n",
    "    \n",
    "    print(f\"\\nðŸ”ª Testing chunking on: {test_type.upper()}\")\n",
    "    print(f\"   Document: {test_metadata.filename}\")\n",
    "    print(f\"   Tokens: {token_stats[test_type]['tokens']:,}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"TEST 3: Hierarchical Chunking\")\n",
    "    print('=' * 60)\n",
    "    \n",
    "    hierarchical_chunker = HierarchicalChunker(parent_chunk_size = 2048,\n",
    "                                               child_chunk_size  = 512,\n",
    "                                               overlap           = 50,\n",
    "                                              )\n",
    "    \n",
    "    hierarchical_chunks  = hierarchical_chunker.chunk_text(text     = test_text, \n",
    "                                                           metadata = test_metadata,\n",
    "                                                          )\n",
    "    \n",
    "    hierarchical_stats   = hierarchical_chunker.get_chunk_statistics(chunks = hierarchical_chunks)\n",
    "    \n",
    "    print(f\"âœ“ Created {len(hierarchical_chunks)} child chunks\")\n",
    "    print(f\"  Total Tokens: {hierarchical_stats['total_tokens']:,}\")\n",
    "    print(f\"  Avg Tokens/Chunk: {hierarchical_stats['avg_tokens_per_chunk']:.1f}\")\n",
    "    \n",
    "    # Get parent-child relationships\n",
    "    relationships = hierarchical_chunker.get_parent_child_relationships(chunks = hierarchical_chunks)\n",
    "    print(f\"  Parent Chunks: {len(relationships)}\")\n",
    "    \n",
    "    if hierarchical_chunks:\n",
    "        print(f\"\\nðŸ“„ First child chunk:\")\n",
    "        chunk = hierarchical_chunks[0]\n",
    "        print(f\"  ID: {chunk.chunk_id}\")\n",
    "        print(f\"  Parent: {chunk.metadata.get('parent_chunk_id', 'N/A')}\")\n",
    "        print(f\"  Tokens: {chunk.token_count}\")\n",
    "        print(f\"  Text: {chunk.text}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17012845-f412-45c0-b3df-788ed8be9168",
   "metadata": {},
   "source": [
    "#### Test 4: Adaptive Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaca74c-9f5c-4f76-a5e4-69f4572e7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first available document\n",
    "if parsed_documents:\n",
    "    test_type     = list(parsed_documents.keys())[0]\n",
    "    test_data     = parsed_documents[test_type]\n",
    "    test_text     = test_data['text']\n",
    "    test_metadata = test_data['metadata']\n",
    "    \n",
    "    print(f\"\\nðŸ”ª Testing chunking on: {test_type.upper()}\")\n",
    "    print(f\"   Document: {test_metadata.filename}\")\n",
    "    print(f\"   Tokens: {token_stats[test_type]['tokens']:,}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"TEST 4: Adaptive Chunking Selector\")\n",
    "    print('=' * 60)\n",
    "    \n",
    "    adaptive_selector          = AdaptiveChunkingSelector()\n",
    "    \n",
    "    # Analyze document\n",
    "    analysis                   = adaptive_selector._analyze_document(text     = test_text, \n",
    "                                                                     metadata = test_metadata,\n",
    "                                                                    )\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Document Analysis:\")\n",
    "    print(f\"  Total Tokens: {analysis['total_tokens']:,}\")\n",
    "    print(f\"  Size Category: {analysis['document_size_category']}\")\n",
    "    print(f\"  Sentences: {analysis['sentence_count']:,}\")\n",
    "    print(f\"  Paragraphs: {analysis['paragraph_count']:,}\")\n",
    "    \n",
    "    # Get strategy recommendation\n",
    "    strategy, analysis_result = adaptive_selector.select_chunking_strategy(text     = test_text, \n",
    "                                                                           metadata = test_metadata,\n",
    "                                                                          )\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Recommended Strategy: {strategy.value.upper()}\")\n",
    "    print(f\"   Reason: {analysis_result['selection_reason']}\")\n",
    "    \n",
    "    # Apply adaptive chunking\n",
    "    adaptive_chunks           = adaptive_selector.chunk_text(text     = test_text, \n",
    "                                                             metadata = test_metadata,\n",
    "                                                            )\n",
    "    \n",
    "    adaptive_stats            = adaptive_selector.fixed_chunker.get_chunk_statistics(chunks = adaptive_chunks)\n",
    "    \n",
    "    print(f\"\\nâœ“ Adaptive chunking completed\")\n",
    "    print(f\"  Chunks Created: {len(adaptive_chunks)}\")\n",
    "    print(f\"  Total Tokens: {adaptive_stats['total_tokens']:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e7208-2714-445b-b6e0-2ceca1129de9",
   "metadata": {},
   "source": [
    "### OVERLAP MANAGEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac9a60-5c85-4864-83db-3a3fff79f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERLAP MANAGEMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if parsed_documents and fixed_chunks:\n",
    "    overlap_manager = OverlapManager(overlap_tokens = 50)\n",
    "    \n",
    "    # Get overlap statistics\n",
    "    overlap_stats   = overlap_manager.get_overlap_statistics(chunks = fixed_chunks)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Overlap Statistics:\")\n",
    "    print(f\"  Number of Chunks: {overlap_stats['num_chunks']}\")\n",
    "    print(f\"  Number of Overlaps: {overlap_stats['num_overlaps']}\")\n",
    "    print(f\"  Avg Overlap Tokens: {overlap_stats['avg_overlap_tokens']:.1f}\")\n",
    "    print(f\"  Min Overlap Tokens: {overlap_stats['min_overlap_tokens']}\")\n",
    "    print(f\"  Max Overlap Tokens: {overlap_stats['max_overlap_tokens']}\")\n",
    "    print(f\"  Avg Overlap %: {overlap_stats['avg_overlap_percentage']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893277a5-1482-428c-959e-d9f8a5c66e1c",
   "metadata": {},
   "source": [
    "### EMBEDDINGS MODULE TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c76c29-4350-437c-9e18-24b2d10519f8",
   "metadata": {},
   "source": [
    "#### TEST MODEL LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ccf67b-6f1c-4f13-bd63-6ece71cc8246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDINGS MODULE TESTING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EMBEDDINGS MODULE TESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Import embeddings module\n",
    "try:\n",
    "    from embeddings.bge_embedder import get_embedder\n",
    "    from embeddings.embedding_cache import get_embedding_cache\n",
    "    from embeddings.model_loader import get_model_loader\n",
    "    from embeddings.batch_processor import get_batch_processor\n",
    "    print(\"âœ“ Embeddings modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— Failed to import embeddings modules: {e}\")\n",
    "    raise\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. TEST MODEL LOADER\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 1: Model Loader\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    model_loader = get_model_loader()\n",
    "    print(\"âœ“ Model loader initialized\")\n",
    "    \n",
    "    # Test model loading\n",
    "    model = model_loader.load_model(device=\"cpu\")\n",
    "    model_info = model_loader.get_model_info()\n",
    "    \n",
    "    print(f\"âœ“ Model loaded successfully:\")\n",
    "    print(f\"  Model: {model_info.get('model_name', 'unknown')}\")\n",
    "    print(f\"  Device: {model_info.get('device', 'unknown')}\")\n",
    "    print(f\"  Embedding dimension: {model_info.get('embedding_dimension', 'unknown')}\")\n",
    "    print(f\"  Cache size: {model_info.get('cache_size', 'unknown')}\")\n",
    "    \n",
    "    # Test model unloading\n",
    "    model_loader.unload_model()\n",
    "    print(\"âœ“ Model unloaded successfully\")\n",
    "    \n",
    "    # Reload for further tests\n",
    "    model = model_loader.load_model()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Model loader test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. TEST BATCH PROCESSOR\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 2: Batch Processor\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    batch_processor = get_batch_processor()\n",
    "    print(\"âœ“ Batch processor initialized\")\n",
    "    \n",
    "    # Test batch splitting\n",
    "    test_texts = [\"Sample text \" + str(i) for i in range(50)]\n",
    "    batches = batch_processor.split_into_optimal_batches(\n",
    "        texts=test_texts,\n",
    "        target_batch_size=10\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Batch splitting successful:\")\n",
    "    print(f\"  Input texts: {len(test_texts)}\")\n",
    "    print(f\"  Created batches: {len(batches)}\")\n",
    "    print(f\"  Batch sizes: {[len(b) for b in batches][:5]}...\")  # Show first 5\n",
    "    \n",
    "    # Test processing stats\n",
    "    stats = batch_processor.get_processing_stats()\n",
    "    print(f\"  Processing stats: {stats}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Batch processor test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. TEST BGE EMBEDDER\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 3: BGE Embedder\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    embedder = get_embedder()\n",
    "    print(\"âœ“ BGE embedder initialized\")\n",
    "    \n",
    "    # Get model info\n",
    "    model_info = embedder.get_model_info()\n",
    "    print(f\"âœ“ Embedder info:\")\n",
    "    print(f\"  Model: {model_info.get('model_name', 'unknown')}\")\n",
    "    print(f\"  Dimension: {model_info.get('embedding_dim', 'unknown')}\")\n",
    "    print(f\"  Device: {model_info.get('device', 'unknown')}\")\n",
    "    print(f\"  Supports batch: {model_info.get('supports_batch', False)}\")\n",
    "    \n",
    "    # Test single text embedding\n",
    "    print(\"\\nTesting single text embedding...\")\n",
    "    test_text = \"This is a test document for embedding generation\"\n",
    "    single_embedding = embedder.embed_text(test_text, normalize=True)\n",
    "    \n",
    "    print(f\"âœ“ Single embedding generated:\")\n",
    "    print(f\"  Text length: {len(test_text)} chars\")\n",
    "    print(f\"  Embedding shape: {single_embedding.shape}\")\n",
    "    print(f\"  Embedding dtype: {single_embedding.dtype}\")\n",
    "    print(f\"  Embedding norm: {np.linalg.norm(single_embedding):.6f}\")\n",
    "    \n",
    "    # Validate embedding\n",
    "    is_valid = embedder.validate_embedding(single_embedding)\n",
    "    print(f\"  Embedding valid: {is_valid}\")\n",
    "    \n",
    "    # Test batch embedding\n",
    "    print(\"\\nTesting batch embedding...\")\n",
    "    test_texts_batch = [\n",
    "        \"First document for testing embeddings\",\n",
    "        \"Second document with different content\",\n",
    "        \"Third document for comprehensive testing\",\n",
    "        \"Fourth document to verify batch processing\",\n",
    "        \"Fifth document to complete the test set\"\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    batch_embeddings = embedder.embed_texts(\n",
    "        texts=test_texts_batch,\n",
    "        batch_size=2,\n",
    "        normalize=True\n",
    "    )\n",
    "    batch_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    print(f\"âœ“ Batch embedding successful:\")\n",
    "    print(f\"  Texts embedded: {len(batch_embeddings)}\")\n",
    "    print(f\"  Time taken: {batch_time:.0f}ms\")\n",
    "    print(f\"  Avg time per text: {batch_time/len(batch_embeddings):.0f}ms\")\n",
    "    \n",
    "    # Verify batch embeddings\n",
    "    valid_count = sum(1 for emb in batch_embeddings if embedder.validate_embedding(emb))\n",
    "    print(f\"  Valid embeddings: {valid_count}/{len(batch_embeddings)}\")\n",
    "    \n",
    "    # Test cosine similarity\n",
    "    print(\"\\nTesting cosine similarity...\")\n",
    "    if len(batch_embeddings) >= 2:\n",
    "        similarity = embedder.cosine_similarity(batch_embeddings[0], batch_embeddings[1])\n",
    "        print(f\"âœ“ Cosine similarity between first two embeddings: {similarity:.4f}\")\n",
    "        \n",
    "        # Self-similarity should be ~1.0\n",
    "        self_similarity = embedder.cosine_similarity(batch_embeddings[0], batch_embeddings[0])\n",
    "        print(f\"  Self-similarity (should be ~1.0): {self_similarity:.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— BGE embedder test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. TEST EMBEDDING CACHE\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 4: Embedding Cache\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    cache = get_embedding_cache()\n",
    "    print(\"âœ“ Embedding cache initialized\")\n",
    "    \n",
    "    # Get initial stats\n",
    "    initial_stats = cache.get_stats()\n",
    "    print(f\"âœ“ Initial cache stats:\")\n",
    "    print(f\"  Cache size: {initial_stats.get('cache_size', 0)}\")\n",
    "    print(f\"  Max size: {initial_stats.get('max_size', 0)}\")\n",
    "    print(f\"  Hits: {initial_stats.get('hits', 0)}\")\n",
    "    print(f\"  Misses: {initial_stats.get('misses', 0)}\")\n",
    "    \n",
    "    # Test cache operations\n",
    "    print(\"\\nTesting cache operations...\")\n",
    "    \n",
    "    # Test single embedding caching\n",
    "    cache_text = \"This text will be cached\"\n",
    "    cache_embedding = np.random.randn(embedder.get_embedding_dimension()).astype(np.float32)\n",
    "    \n",
    "    # Store in cache\n",
    "    cache.set_embedding(cache_text, cache_embedding)\n",
    "    \n",
    "    # Retrieve from cache\n",
    "    retrieved = cache.get_embedding(cache_text)\n",
    "    \n",
    "    if retrieved is not None:\n",
    "        print(f\"âœ“ Single embedding caching successful\")\n",
    "        print(f\"  Retrieved shape: {retrieved.shape}\")\n",
    "        print(f\"  Arrays equal: {np.allclose(cache_embedding, retrieved, atol=1e-6)}\")\n",
    "    \n",
    "    # Test batch caching\n",
    "    print(\"\\nTesting batch caching...\")\n",
    "    batch_texts = [\"Batch text \" + str(i) for i in range(3)]\n",
    "    batch_embeddings = [np.random.randn(embedder.get_embedding_dimension()).astype(np.float32) for _ in range(3)]\n",
    "    \n",
    "    cache.batch_set_embeddings(batch_texts, batch_embeddings)\n",
    "    \n",
    "    cached_batch, missing = cache.batch_get_embeddings(batch_texts)\n",
    "    \n",
    "    print(f\"âœ“ Batch caching successful:\")\n",
    "    print(f\"  Texts cached: {len(batch_texts)}\")\n",
    "    print(f\"  Retrieved: {sum(1 for emb in cached_batch if emb is not None)}\")\n",
    "    print(f\"  Missing: {len(missing)}\")\n",
    "    \n",
    "    # Test smart caching with embed function\n",
    "    print(\"\\nTesting smart caching with embed function...\")\n",
    "    \n",
    "    def mock_embed_function(texts, batch_size=None):\n",
    "        \"\"\"Mock embedding function for testing\"\"\"\n",
    "        return [np.random.randn(embedder.get_embedding_dimension()).astype(np.float32) for _ in texts]\n",
    "    \n",
    "    # First call should miss cache\n",
    "    start_time = time.time()\n",
    "    embeddings1 = cache.get_cached_embeddings(\n",
    "        texts=batch_texts,\n",
    "        embed_function=mock_embed_function,\n",
    "        batch_size=2\n",
    "    )\n",
    "    time1 = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Second call should hit cache\n",
    "    start_time = time.time()\n",
    "    embeddings2 = cache.get_cached_embeddings(\n",
    "        texts=batch_texts,\n",
    "        embed_function=mock_embed_function,\n",
    "        batch_size=2\n",
    "    )\n",
    "    time2 = (time.time() - start_time) * 1000\n",
    "    \n",
    "    print(f\"âœ“ Smart caching test:\")\n",
    "    print(f\"  First call time: {time1:.2f}ms (should be slower)\")\n",
    "    print(f\"  Second call time: {time2:.2f}ms (should be faster)\")\n",
    "    print(f\"  Speedup: {time1/time2:.1f}x\")\n",
    "    \n",
    "    # Verify embeddings are the same\n",
    "    all_same = all(np.allclose(e1, e2, atol=1e-6) for e1, e2 in zip(embeddings1, embeddings2))\n",
    "    print(f\"  Cached embeddings consistent: {all_same}\")\n",
    "    \n",
    "    # Get final stats\n",
    "    final_stats = cache.get_stats()\n",
    "    hit_rate = final_stats.get('hit_rate_percentage', 0)\n",
    "    print(f\"\\nâœ“ Final cache stats:\")\n",
    "    print(f\"  Cache size: {final_stats.get('cache_size', 0)}\")\n",
    "    print(f\"  Hits: {final_stats.get('hits', 0)}\")\n",
    "    print(f\"  Misses: {final_stats.get('misses', 0)}\")\n",
    "    print(f\"  Hit rate: {hit_rate:.1f}%\")\n",
    "    print(f\"  Embeddings generated: {final_stats.get('embeddings_generated', 0)}\")\n",
    "    \n",
    "    # Test cache clearing\n",
    "    cache.clear()\n",
    "    after_clear_stats = cache.get_stats()\n",
    "    print(f\"  After clear - Cache size: {after_clear_stats.get('cache_size', 0)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Embedding cache test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5. TEST INTEGRATION WITH CHUNKS\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 5: Integration with Document Chunks\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'fixed_chunks' in locals() and fixed_chunks:\n",
    "    try:\n",
    "        # Use actual chunks from previous testing\n",
    "        test_chunks = fixed_chunks[:3]  # Use first 3 chunks\n",
    "        \n",
    "        print(f\"Testing embedding generation on {len(test_chunks)} document chunks...\")\n",
    "        \n",
    "        # Generate embeddings for chunks\n",
    "        start_time = time.time()\n",
    "        embedded_chunks = embedder.embed_chunks(\n",
    "            chunks=test_chunks,\n",
    "            batch_size=2,\n",
    "            normalize=True\n",
    "        )\n",
    "        embedding_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        print(f\"âœ“ Document chunk embedding successful:\")\n",
    "        print(f\"  Chunks embedded: {len(embedded_chunks)}\")\n",
    "        print(f\"  Time taken: {embedding_time:.0f}ms\")\n",
    "        print(f\"  Avg time per chunk: {embedding_time/len(embedded_chunks):.0f}ms\")\n",
    "        \n",
    "        # Verify chunks have embeddings\n",
    "        chunks_with_embeddings = sum(1 for chunk in embedded_chunks if hasattr(chunk, 'embedding') and chunk.embedding is not None)\n",
    "        print(f\"  Chunks with embeddings: {chunks_with_embeddings}/{len(embedded_chunks)}\")\n",
    "        \n",
    "        if chunks_with_embeddings > 0:\n",
    "            # Show first chunk details\n",
    "            first_chunk = embedded_chunks[0]\n",
    "            print(f\"\\nðŸ“„ First chunk details:\")\n",
    "            print(f\"  Chunk ID: {first_chunk.chunk_id}\")\n",
    "            print(f\"  Text length: {len(first_chunk.text)} chars\")\n",
    "            print(f\"  Has embedding: {hasattr(first_chunk, 'embedding') and first_chunk.embedding is not None}\")\n",
    "            \n",
    "            if hasattr(first_chunk, 'embedding') and first_chunk.embedding is not None:\n",
    "                embedding_array = np.array(first_chunk.embedding)\n",
    "                print(f\"  Embedding shape: {embedding_array.shape}\")\n",
    "                print(f\"  Embedding dtype: {embedding_array.dtype}\")\n",
    "                print(f\"  Embedding norm: {np.linalg.norm(embedding_array):.6f}\")\n",
    "        \n",
    "        # Test with cache integration\n",
    "        print(\"\\nTesting chunk embedding with cache...\")\n",
    "        \n",
    "        # Clear cache for clean test\n",
    "        cache.clear()\n",
    "        \n",
    "        # First embedding (should miss cache)\n",
    "        start_time = time.time()\n",
    "        embedded_chunks_1 = embedder.embed_chunks(test_chunks, batch_size=2)\n",
    "        time_1 = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Second embedding (should hit cache if using cache)\n",
    "        start_time = time.time()\n",
    "        embedded_chunks_2 = embedder.embed_chunks(test_chunks, batch_size=2)\n",
    "        time_2 = (time.time() - start_time) * 1000\n",
    "        \n",
    "        print(f\"  First embedding time: {time_1:.0f}ms\")\n",
    "        print(f\"  Second embedding time: {time_2:.0f}ms\")\n",
    "        \n",
    "        if time_2 < time_1:\n",
    "            print(f\"  Cache likely working (speedup: {time_1/time_2:.1f}x)\")\n",
    "        else:\n",
    "            print(f\"  Note: No speedup observed (cache may not be integrated in embed_chunks)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Chunk integration test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "else:\n",
    "    print(\"âš ï¸  No chunks available for integration test\")\n",
    "    print(\"   Run chunking tests first or create test chunks\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6. PERFORMANCE BENCHMARKING\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 6: Performance Benchmarking\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create test data of varying sizes\n",
    "    test_sizes = [1, 5, 10, 20, 50]\n",
    "    test_texts_varying = []\n",
    "    \n",
    "    for i in range(max(test_sizes)):\n",
    "        test_texts_varying.append(f\"Test document number {i} with some content to embed. \" * 10)\n",
    "    \n",
    "    print(\"Benchmarking embedding performance...\")\n",
    "    print(f\"{'Texts':<10} {'Time (ms)':<12} {'Time/Text (ms)':<15} {'Memory':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in test_sizes:\n",
    "        texts = test_texts_varying[:size]\n",
    "        \n",
    "        # Clear cache for fair benchmark\n",
    "        cache.clear()\n",
    "        \n",
    "        # Time embedding\n",
    "        start_time = time.time()\n",
    "        embeddings = embedder.embed_texts(texts, batch_size=min(32, size))\n",
    "        elapsed = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Calculate metrics\n",
    "        per_text = elapsed / size if size > 0 else 0\n",
    "        \n",
    "        # Rough memory estimate\n",
    "        if embeddings and len(embeddings) > 0:\n",
    "            # Estimate memory: 4 bytes per float * dimensions * number of embeddings\n",
    "            memory_kb = (embeddings[0].nbytes * len(embeddings)) / 1024\n",
    "        else:\n",
    "            memory_kb = 0\n",
    "        \n",
    "        print(f\"{size:<10} {elapsed:<12.1f} {per_text:<15.2f} {memory_kb:<10.1f}KB\")\n",
    "    \n",
    "    print(\"\\nâœ“ Performance benchmarking complete\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Performance benchmarking failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7. ERROR HANDLING TESTING\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 7: Error Handling\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    print(\"Testing error scenarios...\")\n",
    "    \n",
    "    # Test empty text\n",
    "    try:\n",
    "        empty_result = embedder.embed_text(\"\")\n",
    "        print(\"âœ— Should have raised error for empty text\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ“ Correctly raised error for empty text: {type(e).__name__}\")\n",
    "    \n",
    "    # Test None text\n",
    "    try:\n",
    "        none_result = embedder.embed_text(None)\n",
    "        print(\"âœ— Should have raised error for None text\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ“ Correctly raised error for None text: {type(e).__name__}\")\n",
    "    \n",
    "    # Test invalid batch size\n",
    "    try:\n",
    "        invalid_batch = embedder.embed_texts([\"test\"], batch_size=0)\n",
    "        print(\"âœ— Should have handled invalid batch size\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ“ Correctly handled invalid batch size: {type(e).__name__}\")\n",
    "    \n",
    "    print(\"âœ“ Error handling tests passed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error handling test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EMBEDDINGS MODULE TEST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get final component stats\n",
    "try:\n",
    "    model_stats = model_loader.get_model_info()\n",
    "    batch_stats = batch_processor.get_processing_stats()\n",
    "    cache_stats = cache.get_stats()\n",
    "    embedder_info = embedder.get_model_info()\n",
    "    \n",
    "    print(\"\\nðŸ“Š FINAL COMPONENT STATISTICS:\")\n",
    "    print(f\"\\nModel Loader:\")\n",
    "    print(f\"  Model: {model_stats.get('model_name', 'unknown')}\")\n",
    "    print(f\"  Loaded: {model_stats.get('loaded', False)}\")\n",
    "    print(f\"  Cache size: {model_stats.get('cache_size', 0)}\")\n",
    "    \n",
    "    print(f\"\\nBatch Processor:\")\n",
    "    print(f\"  Total batches: {batch_stats.get('total_batches', 0)}\")\n",
    "    print(f\"  Total texts: {batch_stats.get('total_texts', 0)}\")\n",
    "    print(f\"  Failed batches: {batch_stats.get('failed_batches', 0)}\")\n",
    "    print(f\"  Success rate: {batch_stats.get('success_rate', 0):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nEmbedder:\")\n",
    "    print(f\"  Model: {embedder_info.get('model_name', 'unknown')}\")\n",
    "    print(f\"  Dimension: {embedder_info.get('embedding_dim', 'unknown')}\")\n",
    "    print(f\"  Device: {embedder_info.get('device', 'unknown')}\")\n",
    "    \n",
    "    print(f\"\\nEmbedding Cache:\")\n",
    "    print(f\"  Cache size: {cache_stats.get('cache_size', 0)}\")\n",
    "    print(f\"  Max size: {cache_stats.get('max_size', 0)}\")\n",
    "    print(f\"  Hits: {cache_stats.get('hits', 0)}\")\n",
    "    print(f\"  Misses: {cache_stats.get('misses', 0)}\")\n",
    "    print(f\"  Hit rate: {cache_stats.get('hit_rate_percentage', 0):.1f}%\")\n",
    "    print(f\"  Embeddings generated: {cache_stats.get('embeddings_generated', 0)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting final stats: {e}\")\n",
    "\n",
    "print(\"\\nâœ… EMBEDDINGS MODULE TESTING COMPLETE\")\n",
    "print(\"   Next step: Vector Store Module Testing\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3d70b-bcd2-473e-a59a-84e52915b411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef929a-a6b5-461d-841f-d6616c016368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991566f-403e-4135-a6af-c7f616bf7f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e9336-4070-49fb-a5a3-6b3867d1a70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b3b8a-8a1b-4e99-9012-2e36d7f0f127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8ade0-31af-49b7-8ae4-39ddfe4ff97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58a470-56c6-4694-b40e-6eb038539d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d8a968d-73e5-4502-bd9f-5ce6336c842f",
   "metadata": {},
   "source": [
    "### VECTOR STORE MODULE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec362a-9b99-4604-9bc0-b5db42edb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create temporary directory for testing\n",
    "test_temp_dir = Path(tempfile.mkdtemp(prefix=\"vector_store_test_\"))\n",
    "print(f\"\\nðŸ“ Using temporary directory: {test_temp_dir}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. TEST BACKUP MANAGER\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 1: Backup Manager\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create test directory structure\n",
    "    test_vector_store = test_temp_dir / \"vector_store\"\n",
    "    test_backup_dir = test_temp_dir / \"backups\"\n",
    "    test_vector_store.mkdir(exist_ok=True)\n",
    "    test_backup_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize backup manager\n",
    "    backup_manager = BackupManager(\n",
    "        backup_dir=test_backup_dir,\n",
    "        vector_store_dir=test_vector_store\n",
    "    )\n",
    "    print(\"âœ“ Backup manager initialized\")\n",
    "    \n",
    "    # Create some test files in vector store\n",
    "    test_files = [\"faiss.index\", \"bm25_index.pkl\", \"metadata.db\"]\n",
    "    for file_name in test_files:\n",
    "        test_file = test_vector_store / file_name\n",
    "        test_file.write_text(f\"Test content for {file_name}\")\n",
    "    \n",
    "    # Test backup creation\n",
    "    print(\"\\nTesting backup creation...\")\n",
    "    backup_path = backup_manager.create_backup(\n",
    "        backup_name=\"test_backup\",\n",
    "        description=\"Test backup for integration testing\"\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Backup created: {Path(backup_path).name}\")\n",
    "    \n",
    "    # Test backup listing\n",
    "    print(\"\\nTesting backup listing...\")\n",
    "    backups = backup_manager.list_backups()\n",
    "    print(f\"âœ“ Found {len(backups)} backups\")\n",
    "    \n",
    "    if backups:\n",
    "        backup_info = backups[0]\n",
    "        print(f\"  Latest backup: {backup_info['name']}\")\n",
    "        print(f\"  Size: {backup_info['size_mb']:.2f} MB\")\n",
    "    \n",
    "    # Test backup verification\n",
    "    print(\"\\nTesting backup verification...\")\n",
    "    is_valid = backup_manager.verify_backup(Path(backup_path))\n",
    "    print(f\"âœ“ Backup verification: {'Valid' if is_valid else 'Invalid'}\")\n",
    "    \n",
    "    # Test backup statistics\n",
    "    print(\"\\nTesting backup statistics...\")\n",
    "    backup_stats = backup_manager.get_backup_stats()\n",
    "    print(f\"âœ“ Backup stats:\")\n",
    "    print(f\"  Total backups: {backup_stats.get('total_backups', 0)}\")\n",
    "    print(f\"  Total size: {backup_stats.get('total_size_mb', 0):.2f} MB\")\n",
    "    print(f\"  Auto backup: {backup_stats.get('auto_backup', False)}\")\n",
    "    \n",
    "    # Test cleanup old backups\n",
    "    print(\"\\nTesting backup cleanup...\")\n",
    "    cleanup_result = backup_manager.cleanup_old_backups(keep_count=2, keep_days=1)\n",
    "    print(f\"âœ“ Backup cleanup: {cleanup_result.get('message', 'Unknown')}\")\n",
    "    print(f\"  Deleted: {cleanup_result.get('deleted', 0)}\")\n",
    "    print(f\"  Kept: {cleanup_result.get('kept', 0)}\")\n",
    "    \n",
    "    # Test auto backup check\n",
    "    print(\"\\nTesting auto backup check...\")\n",
    "    auto_backup_result = backup_manager.auto_backup_check(documents_processed=50)\n",
    "    if auto_backup_result:\n",
    "        print(f\"âœ“ Auto backup triggered: {Path(auto_backup_result).name}\")\n",
    "    else:\n",
    "        print(f\"âœ“ Auto backup not triggered (interval not reached)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Backup manager test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. TEST INDEX PERSISTER\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 2: Index Persister\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create test directory for persister\n",
    "    test_persister_dir = test_temp_dir / \"persister_test\"\n",
    "    test_persister_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize index persister\n",
    "    index_persister = IndexPersister(vector_store_dir=test_persister_dir)\n",
    "    print(\"âœ“ Index persister initialized\")\n",
    "    \n",
    "    # Test file existence check\n",
    "    print(\"\\nTesting index file existence...\")\n",
    "    files_exist = index_persister.index_files_exist()\n",
    "    print(f\"âœ“ Index files exist: {files_exist}\")\n",
    "    \n",
    "    # Test file info\n",
    "    print(\"\\nTesting file information...\")\n",
    "    files_info = index_persister.get_index_files_info()\n",
    "    print(f\"âœ“ File info retrieved for {len(files_info)} files\")\n",
    "    \n",
    "    for file_name, info in files_info.items():\n",
    "        exists = info.get('exists', False)\n",
    "        print(f\"  {file_name}: {'Exists' if exists else 'Missing'}\")\n",
    "    \n",
    "    # Test persistence statistics\n",
    "    print(\"\\nTesting persistence statistics...\")\n",
    "    persistence_stats = index_persister.get_persistence_stats()\n",
    "    print(f\"âœ“ Persistence stats:\")\n",
    "    print(f\"  Total size: {persistence_stats.get('total_size_mb', 0):.2f} MB\")\n",
    "    print(f\"  File count: {persistence_stats.get('file_count', 0)}\")\n",
    "    \n",
    "    # Test metadata saving/loading\n",
    "    print(\"\\nTesting metadata operations...\")\n",
    "    test_metadata = {\n",
    "        \"test_key\": \"test_value\",\n",
    "        \"timestamp\": \"2024-01-01T12:00:00\",\n",
    "        \"count\": 42\n",
    "    }\n",
    "    \n",
    "    save_success = index_persister.save_index_metadata(test_metadata, \"test_metadata.json\")\n",
    "    print(f\"âœ“ Metadata save: {'Success' if save_success else 'Failed'}\")\n",
    "    \n",
    "    loaded_metadata = index_persister.load_index_metadata(\"test_metadata.json\")\n",
    "    print(f\"âœ“ Metadata load: {len(loaded_metadata)} keys loaded\")\n",
    "    \n",
    "    if loaded_metadata:\n",
    "        print(f\"  Loaded key 'test_key': {loaded_metadata.get('test_key', 'Not found')}\")\n",
    "    \n",
    "    # Test cleanup (basic)\n",
    "    print(\"\\nTesting index cleanup...\")\n",
    "    cleanup_result = index_persister.cleanup_old_indexes(keep_latest=True)\n",
    "    print(f\"âœ“ Index cleanup: {cleanup_result.get('message', 'Unknown')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Index persister test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. TEST METADATA STORE\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 3: Metadata Store\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create test database\n",
    "    test_db_path = test_temp_dir / \"test_metadata.db\"\n",
    "    \n",
    "    # Initialize metadata store\n",
    "    metadata_store = MetadataStore(db_path=test_db_path)\n",
    "    print(\"âœ“ Metadata store initialized\")\n",
    "    \n",
    "    # Test database readiness\n",
    "    print(\"\\nTesting database readiness...\")\n",
    "    is_ready = metadata_store.is_ready()\n",
    "    print(f\"âœ“ Database ready: {is_ready}\")\n",
    "    \n",
    "    # Create test chunks\n",
    "    print(\"\\nCreating test chunks...\")\n",
    "    from config.models import DocumentChunk\n",
    "    from datetime import datetime\n",
    "    \n",
    "    test_chunks = []\n",
    "    for i in range(5):\n",
    "        chunk = DocumentChunk(\n",
    "            chunk_id=f\"chunk_test_{i}\",\n",
    "            document_id=f\"doc_test_{i // 2}\",  # 2 chunks per document\n",
    "            text=f\"This is test chunk {i} with some content for metadata testing.\",\n",
    "            embedding=[0.1 * j for j in range(384)],  # Mock embedding\n",
    "            chunk_index=i,\n",
    "            start_char=i * 100,\n",
    "            end_char=(i + 1) * 100,\n",
    "            page_number=1,\n",
    "            section_title=f\"Section {i}\",\n",
    "            token_count=50,\n",
    "            metadata={\n",
    "                \"source\": \"test\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"test_field\": f\"value_{i}\"\n",
    "            }\n",
    "        )\n",
    "        test_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"âœ“ Created {len(test_chunks)} test chunks\")\n",
    "    \n",
    "    # Test chunk storage\n",
    "    print(\"\\nTesting chunk storage...\")\n",
    "    storage_result = metadata_store.store_chunks(test_chunks, rebuild=True)\n",
    "    print(f\"âœ“ Chunk storage result:\")\n",
    "    print(f\"  Stored chunks: {storage_result.get('stored_chunks', 0)}\")\n",
    "    print(f\"  Stored documents: {storage_result.get('stored_documents', 0)}\")\n",
    "    \n",
    "    # Test getting chunk metadata\n",
    "    print(\"\\nTesting chunk metadata retrieval...\")\n",
    "    if test_chunks:\n",
    "        chunk_metadata = metadata_store.get_chunk_metadata(test_chunks[0].chunk_id)\n",
    "        print(f\"âœ“ Chunk metadata retrieved: {chunk_metadata is not None}\")\n",
    "        if chunk_metadata:\n",
    "            print(f\"  Chunk ID: {chunk_metadata.get('chunk_id', 'Unknown')}\")\n",
    "            print(f\"  Document ID: {chunk_metadata.get('document_id', 'Unknown')}\")\n",
    "            print(f\"  Text preview: {chunk_metadata.get('text', '')[:50]}...\")\n",
    "    \n",
    "    # Test getting chunks by document\n",
    "    print(\"\\nTesting chunks by document...\")\n",
    "    if test_chunks:\n",
    "        doc_chunks = metadata_store.get_chunks_by_document(test_chunks[0].document_id)\n",
    "        print(f\"âœ“ Retrieved {len(doc_chunks)} chunks for document\")\n",
    "    \n",
    "    # Test getting all chunks\n",
    "    print(\"\\nTesting all chunks retrieval...\")\n",
    "    all_chunks = metadata_store.get_all_chunks()\n",
    "    print(f\"âœ“ Retrieved {len(all_chunks)} total chunks\")\n",
    "    \n",
    "    # Test getting document metadata\n",
    "    print(\"\\nTesting document metadata...\")\n",
    "    if test_chunks:\n",
    "        doc_metadata = metadata_store.get_document_metadata(test_chunks[0].document_id)\n",
    "        print(f\"âœ“ Document metadata retrieved: {doc_metadata is not None}\")\n",
    "        if doc_metadata:\n",
    "            print(f\"  Document ID: {doc_metadata.get('document_id', 'Unknown')}\")\n",
    "            print(f\"  Filename: {doc_metadata.get('filename', 'Unknown')}\")\n",
    "            print(f\"  Document type: {doc_metadata.get('document_type', 'Unknown')}\")\n",
    "    \n",
    "    # Test statistics\n",
    "    print(\"\\nTesting metadata store statistics...\")\n",
    "    stats = metadata_store.get_stats()\n",
    "    print(f\"âœ“ Metadata store stats:\")\n",
    "    print(f\"  Documents: {stats.get('documents', 0)}\")\n",
    "    print(f\"  Chunks: {stats.get('chunks', 0)}\")\n",
    "    print(f\"  Database size: {stats.get('database_size_mb', 0):.2f} MB\")\n",
    "    \n",
    "    # Test size information\n",
    "    print(\"\\nTesting size information...\")\n",
    "    size_info = metadata_store.get_size()\n",
    "    print(f\"âœ“ Size info:\")\n",
    "    print(f\"  Disk usage: {size_info.get('disk_mb', 0):.2f} MB\")\n",
    "    \n",
    "    # Test clearing metadata\n",
    "    print(\"\\nTesting metadata clearing...\")\n",
    "    metadata_store.clear()\n",
    "    print(\"âœ“ Metadata cleared\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Metadata store test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. TEST FAISS MANAGER\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 4: FAISS Manager\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create test directory for FAISS\n",
    "    test_faiss_dir = test_temp_dir / \"faiss_test\"\n",
    "    test_faiss_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize FAISS manager\n",
    "    faiss_manager = FAISSManager(vector_store_dir=test_faiss_dir)\n",
    "    print(\"âœ“ FAISS manager initialized\")\n",
    "    \n",
    "    # Create test embeddings and chunk IDs\n",
    "    print(\"\\nCreating test embeddings...\")\n",
    "    embedding_dim = 384  # BGE-small dimension\n",
    "    num_vectors = 100\n",
    "    \n",
    "    # Generate random embeddings\n",
    "    np.random.seed(42)\n",
    "    test_embeddings = np.random.randn(num_vectors, embedding_dim).astype('float32')\n",
    "    \n",
    "    # Normalize embeddings (as BGE embedder would do)\n",
    "    norms = np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "    test_embeddings = test_embeddings / norms\n",
    "    \n",
    "    # Create chunk IDs\n",
    "    test_chunk_ids = [f\"chunk_faiss_{i}\" for i in range(num_vectors)]\n",
    "    \n",
    "    # Test index building\n",
    "    print(\"\\nTesting FAISS index building...\")\n",
    "    build_stats = faiss_manager.build_index(\n",
    "        embeddings=test_embeddings,\n",
    "        chunk_ids=test_chunk_ids,\n",
    "        rebuild=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ FAISS index built:\")\n",
    "    print(f\"  Vectors: {build_stats.get('vectors', 0)}\")\n",
    "    print(f\"  Build time: {build_stats.get('build_time_seconds', 0):.2f}s\")\n",
    "    print(f\"  Index type: {build_stats.get('index_type', 'Unknown')}\")\n",
    "    \n",
    "    # Test index statistics\n",
    "    print(\"\\nTesting FAISS index statistics...\")\n",
    "    index_stats = faiss_manager.get_index_stats()\n",
    "    print(f\"âœ“ FAISS index stats:\")\n",
    "    print(f\"  Built: {index_stats.get('built', False)}\")\n",
    "    print(f\"  Vector count: {index_stats.get('vector_count', 0)}\")\n",
    "    print(f\"  Embedding dim: {index_stats.get('embedding_dim', 0)}\")\n",
    "    print(f\"  Search count: {index_stats.get('search_count', 0)}\")\n",
    "    \n",
    "    # Test search functionality\n",
    "    print(\"\\nTesting FAISS search...\")\n",
    "    # Create a query embedding\n",
    "    query_embedding = np.random.randn(embedding_dim).astype('float32')\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    search_results = faiss_manager.search(\n",
    "        query_embedding=query_embedding,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ FAISS search completed:\")\n",
    "    print(f\"  Results returned: {len(search_results)}\")\n",
    "    if search_results:\n",
    "        print(f\"  Top result: {search_results[0][0]} with score {search_results[0][1]:.4f}\")\n",
    "    \n",
    "    # Test adding to index\n",
    "    print(\"\\nTesting adding to FAISS index...\")\n",
    "    new_embeddings = np.random.randn(10, embedding_dim).astype('float32')\n",
    "    norms = np.linalg.norm(new_embeddings, axis=1, keepdims=True)\n",
    "    new_embeddings = new_embeddings / norms\n",
    "    new_chunk_ids = [f\"chunk_new_{i}\" for i in range(10)]\n",
    "    \n",
    "    add_stats = faiss_manager.add_to_index(new_embeddings, new_chunk_ids)\n",
    "    print(f\"âœ“ Added to FAISS index:\")\n",
    "    print(f\"  Added vectors: {add_stats.get('added', 0)}\")\n",
    "    print(f\"  New total: {add_stats.get('new_total', 0)}\")\n",
    "    \n",
    "    # Test index optimization\n",
    "    print(\"\\nTesting FAISS index optimization...\")\n",
    "    opt_stats = faiss_manager.optimize_index()\n",
    "    print(f\"âœ“ FAISS optimization: {opt_stats.get('message', 'Unknown')}\")\n",
    "    \n",
    "    # Test index size\n",
    "    print(\"\\nTesting FAISS index size...\")\n",
    "    size_info = faiss_manager.get_index_size()\n",
    "    print(f\"âœ“ FAISS size info:\")\n",
    "    print(f\"  Memory: {size_info.get('memory_mb', 0):.2f} MB\")\n",
    "    print(f\"  Disk: {size_info.get('disk_mb', 0):.2f} MB\")\n",
    "    print(f\"  Vector count: {size_info.get('vector_count', 0)}\")\n",
    "    \n",
    "    # Test index readiness\n",
    "    print(\"\\nTesting index readiness...\")\n",
    "    is_built = faiss_manager.is_index_built()\n",
    "    print(f\"âœ“ Index built: {is_built}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— FAISS manager test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5. TEST BM25 INDEX (if available)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 5: BM25 Index\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if BM25_AVAILABLE:\n",
    "    try:\n",
    "        # Initialize BM25 index\n",
    "        bm25_index = BM25Index()\n",
    "        print(\"âœ“ BM25 index initialized\")\n",
    "        \n",
    "        # Create test texts and chunk IDs\n",
    "        print(\"\\nCreating test texts...\")\n",
    "        test_texts = [\n",
    "            \"This is a test document about artificial intelligence and machine learning.\",\n",
    "            \"Machine learning algorithms can learn from data and make predictions.\",\n",
    "            \"Artificial intelligence is transforming many industries today.\",\n",
    "            \"Deep learning is a subset of machine learning using neural networks.\",\n",
    "            \"Natural language processing helps computers understand human language.\",\n",
    "            \"Computer vision enables machines to interpret visual information.\",\n",
    "            \"Reinforcement learning involves agents learning from interactions.\",\n",
    "            \"Supervised learning uses labeled datasets for training models.\",\n",
    "            \"Unsupervised learning finds patterns in unlabeled data.\",\n",
    "            \"Transfer learning applies knowledge from one domain to another.\"\n",
    "        ]\n",
    "        \n",
    "        test_bm25_chunk_ids = [f\"chunk_bm25_{i}\" for i in range(len(test_texts))]\n",
    "        \n",
    "        # Test index building\n",
    "        print(\"\\nTesting BM25 index building...\")\n",
    "        build_stats = bm25_index.build_index(\n",
    "            texts=test_texts,\n",
    "            chunk_ids=test_bm25_chunk_ids,\n",
    "            rebuild=True\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ BM25 index built:\")\n",
    "        print(f\"  Documents: {build_stats.get('documents', 0)}\")\n",
    "        print(f\"  Vocabulary size: {build_stats.get('vocabulary_size', 0)}\")\n",
    "        print(f\"  Build time: {build_stats.get('build_time_seconds', 0):.2f}s\")\n",
    "        \n",
    "        # Test index statistics\n",
    "        print(\"\\nTesting BM25 index statistics...\")\n",
    "        index_stats = bm25_index.get_index_stats()\n",
    "        print(f\"âœ“ BM25 index stats:\")\n",
    "        print(f\"  Built: {index_stats.get('built', False)}\")\n",
    "        print(f\"  Document count: {index_stats.get('document_count', 0)}\")\n",
    "        print(f\"  Vocabulary size: {index_stats.get('vocabulary_size', 0)}\")\n",
    "        print(f\"  Search count: {index_stats.get('search_count', 0)}\")\n",
    "        \n",
    "        # Test search functionality\n",
    "        print(\"\\nTesting BM25 search...\")\n",
    "        test_queries = [\n",
    "            \"machine learning\",\n",
    "            \"artificial intelligence\",\n",
    "            \"neural networks\",\n",
    "            \"computer vision\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            results = bm25_index.search(query=query, top_k=3)\n",
    "            print(f\"  Query: '{query}' -> {len(results)} results\")\n",
    "            if results:\n",
    "                print(f\"    Top: {results[0][0]} (score: {results[0][1]:.4f})\")\n",
    "        \n",
    "        # Test adding to index\n",
    "        print(\"\\nTesting adding to BM25 index...\")\n",
    "        new_texts = [\n",
    "            \"New document about AI ethics and responsible innovation.\",\n",
    "            \"Another document discussing future trends in AI.\"\n",
    "        ]\n",
    "        new_chunk_ids = [f\"chunk_new_{i}\" for i in range(len(new_texts))]\n",
    "        \n",
    "        add_stats = bm25_index.add_to_index(new_texts, new_chunk_ids)\n",
    "        print(f\"âœ“ Added to BM25 index:\")\n",
    "        print(f\"  Added documents: {add_stats.get('added', 0)}\")\n",
    "        print(f\"  New total: {add_stats.get('new_total', 0)}\")\n",
    "        \n",
    "        # Test term statistics\n",
    "        print(\"\\nTesting term statistics...\")\n",
    "        test_terms = [\"learning\", \"artificial\", \"computer\"]\n",
    "        for term in test_terms:\n",
    "            term_stats = bm25_index.get_term_stats(term)\n",
    "            if term_stats:\n",
    "                print(f\"  Term '{term}': freq={term_stats.get('term_frequency', 0)}, \"\n",
    "                      f\"doc_freq={term_stats.get('document_frequency', 0)}\")\n",
    "        \n",
    "        # Test index optimization\n",
    "        print(\"\\nTesting BM25 index optimization...\")\n",
    "        opt_stats = bm25_index.optimize_index()\n",
    "        print(f\"âœ“ BM25 optimization: {opt_stats.get('message', 'Unknown')}\")\n",
    "        \n",
    "        # Test index size\n",
    "        print(\"\\nTesting BM25 index size...\")\n",
    "        size_info = bm25_index.get_index_size()\n",
    "        print(f\"âœ“ BM25 size info:\")\n",
    "        print(f\"  Memory: {size_info.get('memory_mb', 0):.2f} MB\")\n",
    "        print(f\"  Disk: {size_info.get('disk_mb', 0):.2f} MB\")\n",
    "        print(f\"  Document count: {size_info.get('document_count', 0)}\")\n",
    "        \n",
    "        # Test index readiness\n",
    "        print(\"\\nTesting BM25 index readiness...\")\n",
    "        is_built = bm25_index.is_index_built()\n",
    "        print(f\"âœ“ BM25 index built: {is_built}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— BM25 index test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"âš ï¸  BM25 tests skipped (rank_bm25 not available)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6. TEST INDEX BUILDER (Integration Test)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 6: Index Builder (Integration)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create test directory for index builder\n",
    "    test_index_builder_dir = test_temp_dir / \"index_builder_test\"\n",
    "    test_index_builder_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize index builder\n",
    "    index_builder = IndexBuilder(vector_store_dir=test_index_builder_dir)\n",
    "    print(\"âœ“ Index builder initialized\")\n",
    "    \n",
    "    # Create test chunks with embeddings\n",
    "    print(\"\\nCreating test chunks with embeddings...\")\n",
    "    from config.models import DocumentChunk\n",
    "    \n",
    "    test_integration_chunks = []\n",
    "    num_test_chunks = 20\n",
    "    \n",
    "    for i in range(num_test_chunks):\n",
    "        # Create embedding\n",
    "        embedding = np.random.randn(384).astype('float32').tolist()\n",
    "        embedding = [float(x) for x in embedding]  # Ensure Python floats\n",
    "        \n",
    "        chunk = DocumentChunk(\n",
    "            chunk_id=f\"chunk_integration_{i}\",\n",
    "            document_id=f\"doc_integration_{i // 5}\",  # 5 chunks per document\n",
    "            text=f\"This is integration test chunk {i} for the index builder. It contains text about various topics for testing purposes.\",\n",
    "            embedding=embedding,\n",
    "            chunk_index=i,\n",
    "            start_char=i * 100,\n",
    "            end_char=(i + 1) * 100,\n",
    "            page_number=1 + (i // 10),\n",
    "            section_title=f\"Section {i % 3}\",\n",
    "            token_count=75,\n",
    "            metadata={\n",
    "                \"source\": \"integration_test\",\n",
    "                \"timestamp\": \"2024-01-01T12:00:00\",\n",
    "                \"test_id\": i\n",
    "            }\n",
    "        )\n",
    "        test_integration_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"âœ“ Created {len(test_integration_chunks)} test chunks\")\n",
    "    \n",
    "    # Test building all indexes\n",
    "    print(\"\\nTesting index building (FAISS + BM25 + Metadata)...\")\n",
    "    build_stats = index_builder.build_indexes(\n",
    "        chunks=test_integration_chunks,\n",
    "        rebuild=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Index building completed:\")\n",
    "    print(f\"  Total chunks: {build_stats.get('total_chunks', 0)}\")\n",
    "    print(f\"  Build time: {build_stats.get('build_time_seconds', 0):.2f}s\")\n",
    "    print(f\"  Chunks/sec: {build_stats.get('chunks_per_second', 0):.1f}\")\n",
    "    \n",
    "    # Check component stats\n",
    "    faiss_stats = build_stats.get('faiss', {})\n",
    "    bm25_stats = build_stats.get('bm25', {})\n",
    "    metadata_stats = build_stats.get('metadata', {})\n",
    "    \n",
    "    print(f\"\\n  FAISS: {faiss_stats.get('vectors', 0)} vectors\")\n",
    "    print(f\"  BM25: {bm25_stats.get('documents', 0)} documents\")\n",
    "    print(f\"  Metadata: {metadata_stats.get('stored_chunks', 0)} chunks stored\")\n",
    "    \n",
    "    # Test index statistics\n",
    "    print(\"\\nTesting comprehensive index statistics...\")\n",
    "    index_stats = index_builder.get_index_stats()\n",
    "    print(f\"âœ“ Index statistics retrieved\")\n",
    "    \n",
    "    total_chunks = index_stats.get('total_chunks_indexed', 0)\n",
    "    print(f\"  Total chunks indexed: {total_chunks}\")\n",
    "    \n",
    "    # Test index readiness\n",
    "    print(\"\\nTesting index readiness...\")\n",
    "    is_built = index_builder.is_index_built()\n",
    "    print(f\"âœ“ All indexes built: {is_built}\")\n",
    "    \n",
    "    # Test index optimization\n",
    "    print(\"\\nTesting index optimization...\")\n",
    "    opt_stats = index_builder.optimize_indexes()\n",
    "    print(f\"âœ“ Index optimization completed\")\n",
    "    \n",
    "    # Test index size\n",
    "    print(\"\\nTesting index size information...\")\n",
    "    size_info = index_builder.get_index_size()\n",
    "    print(f\"âœ“ Index size info:\")\n",
    "    print(f\"  Total memory: {size_info.get('total_memory_mb', 0):.2f} MB\")\n",
    "    print(f\"  Total disk: {size_info.get('total_disk_mb', 0):.2f} MB\")\n",
    "    \n",
    "    # Test integration with previously tested components\n",
    "    print(\"\\nTesting integration with FAISS manager...\")\n",
    "    faiss_manager = index_builder.faiss_manager\n",
    "    if faiss_manager:\n",
    "        faiss_integration_stats = faiss_manager.get_index_stats()\n",
    "        print(f\"  FAISS integration: {faiss_integration_stats.get('vector_count', 0)} vectors\")\n",
    "    \n",
    "    print(\"\\nTesting integration with metadata store...\")\n",
    "    metadata_integration_stats = index_builder.metadata_store.get_stats()\n",
    "    print(f\"  Metadata integration: {metadata_integration_stats.get('chunks', 0)} chunks\")\n",
    "    \n",
    "    # Test clearing indexes\n",
    "    print(\"\\nTesting index clearing...\")\n",
    "    index_builder.clear_indexes()\n",
    "    print(\"âœ“ Indexes cleared\")\n",
    "    \n",
    "    # Verify indexes are cleared\n",
    "    is_still_built = index_builder.is_index_built()\n",
    "    print(f\"  Indexes after clear: {'Built' if is_still_built else 'Cleared'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Index builder test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7. TEST GLOBAL INSTANCES\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 7: Global Instances\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    print(\"Testing global instance getters...\")\n",
    "    \n",
    "    # Test get_backup_manager\n",
    "    backup_manager_global = get_backup_manager()\n",
    "    print(f\"âœ“ Backup manager global instance: {backup_manager_global is not None}\")\n",
    "    \n",
    "    # Test get_index_persister\n",
    "    index_persister_global = get_index_persister()\n",
    "    print(f\"âœ“ Index persister global instance: {index_persister_global is not None}\")\n",
    "    \n",
    "    # Test get_metadata_store\n",
    "    metadata_store_global = get_metadata_store()\n",
    "    print(f\"âœ“ Metadata store global instance: {metadata_store_global is not None}\")\n",
    "    \n",
    "    # Test get_index_builder\n",
    "    index_builder_global = get_index_builder()\n",
    "    print(f\"âœ“ Index builder global instance: {index_builder_global is not None}\")\n",
    "    \n",
    "    # Test get_faiss_manager\n",
    "    faiss_manager_global = get_faiss_manager()\n",
    "    print(f\"âœ“ FAISS manager global instance: {faiss_manager_global is not None}\")\n",
    "    \n",
    "    # Test get_bm25_index (if available)\n",
    "    if BM25_AVAILABLE:\n",
    "        bm25_index_global = get_bm25_index()\n",
    "        print(f\"âœ“ BM25 index global instance: {bm25_index_global is not None}\")\n",
    "    \n",
    "    print(\"âœ“ All global instances retrieved successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Global instances test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 8. TEST INTEGRATION WITH EMBEDDINGS MODULE\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEST 8: Integration with Embeddings Module\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check if we have embeddings from previous tests\n",
    "if 'embedder' in locals() and 'test_chunks' in locals():\n",
    "    try:\n",
    "        print(\"Testing integration between embeddings and vector store...\")\n",
    "        \n",
    "        # Create a new test directory\n",
    "        test_integration_dir = test_temp_dir / \"embedding_vector_integration\"\n",
    "        test_integration_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Get embedder from previous test\n",
    "        print(\"  Using embedder from previous tests...\")\n",
    "        \n",
    "        # Create test chunks with actual embeddings\n",
    "        print(\"  Creating chunks with actual embeddings...\")\n",
    "        test_texts_for_embedding = [\n",
    "            \"Artificial intelligence is revolutionizing many industries.\",\n",
    "            \"Machine learning algorithms require large datasets for training.\",\n",
    "            \"Natural language processing enables human-computer interaction.\",\n",
    "            \"Computer vision systems can identify objects in images.\",\n",
    "            \"Deep learning uses neural networks with multiple layers.\"\n",
    "        ]\n",
    "        \n",
    "        # Generate embeddings using embedder\n",
    "        embeddings_list = embedder.embed_texts(\n",
    "            texts=test_texts_for_embedding,\n",
    "            batch_size=2,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        print(f\"  Generated {len(embeddings_list)} embeddings\")\n",
    "        \n",
    "        # Create DocumentChunk objects\n",
    "        from config.models import DocumentChunk\n",
    "        \n",
    "        integration_chunks = []\n",
    "        for i, (text, embedding) in enumerate(zip(test_texts_for_embedding, embeddings_list)):\n",
    "            chunk = DocumentChunk(\n",
    "                chunk_id=f\"integration_chunk_{i}\",\n",
    "                document_id=\"integration_doc_1\",\n",
    "                text=text,\n",
    "                embedding=embedding.tolist(),\n",
    "                chunk_index=i,\n",
    "                start_char=i * 100,\n",
    "                end_char=(i + 1) * 100,\n",
    "                page_number=1,\n",
    "                section_title=f\"AI Topic {i}\",\n",
    "                token_count=len(text.split()),\n",
    "                metadata={\n",
    "                    \"source\": \"embedding_integration_test\",\n",
    "                    \"embedding_dim\": len(embedding)\n",
    "                }\n",
    "            )\n",
    "            integration_chunks.append(chunk)\n",
    "        \n",
    "        print(f\"  Created {len(integration_chunks)} chunks with embeddings\")\n",
    "        \n",
    "        # Build indexes using index builder\n",
    "        print(\"  Building indexes with embedded chunks...\")\n",
    "        integration_index_builder = IndexBuilder(vector_store_dir=test_integration_dir)\n",
    "        integration_stats = integration_index_builder.build_indexes(\n",
    "            chunks=integration_chunks,\n",
    "            rebuild=True\n",
    "        )\n",
    "        \n",
    "        print(f\"  Indexes built successfully:\")\n",
    "        print(f\"    Chunks indexed: {integration_stats.get('total_chunks', 0)}\")\n",
    "        print(f\"    FAISS vectors: {integration_stats.get('faiss', {}).get('vectors', 0)}\")\n",
    "        print(f\"    BM25 documents: {integration_stats.get('bm25', {}).get('documents', 0)}\")\n",
    "        \n",
    "        # Test retrieval\n",
    "        print(\"  Testing retrieval integration...\")\n",
    "        \n",
    "        # Create a query embedding\n",
    "        query_text = \"machine learning artificial intelligence\"\n",
    "        query_embedding = embedder.embed_text(query_text, normalize=True)\n",
    "        \n",
    "        # Use FAISS manager to search\n",
    "        faiss_results = integration_index_builder.faiss_manager.search(\n",
    "            query_embedding=query_embedding,\n",
    "            top_k=3\n",
    "        )\n",
    "        \n",
    "        print(f\"  FAISS search results: {len(faiss_results)} matches\")\n",
    "        if faiss_results:\n",
    "            for i, (chunk_id, score) in enumerate(faiss_results[:3]):\n",
    "                print(f\"    Result {i+1}: {chunk_id} (score: {score:.4f})\")\n",
    "        \n",
    "        # Use BM25 to search (if available)\n",
    "        if BM25_AVAILABLE:\n",
    "            bm25_results = integration_index_builder.bm25_index.search(\n",
    "                query=query_text,\n",
    "                top_k=3\n",
    "            )\n",
    "            print(f\"  BM25 search results: {len(bm25_results)} matches\")\n",
    "        \n",
    "        print(\"âœ“ Embedding-vector store integration test successful\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Integration test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"âš ï¸  Embedding integration test skipped (run embedding tests first)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CLEANUP\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"CLEANUP\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Clean up temporary directory\n",
    "    if test_temp_dir.exists():\n",
    "        shutil.rmtree(test_temp_dir)\n",
    "        print(f\"âœ“ Cleaned up temporary directory: {test_temp_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Cleanup warning: {e}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VECTOR STORE MODULE TEST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š TEST RESULTS:\")\n",
    "print(\"1. âœ… Backup Manager: Tested backup creation, listing, verification, and cleanup\")\n",
    "print(\"2. âœ… Index Persister: Tested file persistence, metadata operations, and cleanup\")\n",
    "print(\"3. âœ… Metadata Store: Tested chunk storage, retrieval, statistics, and database operations\")\n",
    "print(\"4. âœ… FAISS Manager: Tested index building, search, optimization, and statistics\")\n",
    "print(\"5. \" + (\"âœ…\" if BM25_AVAILABLE else \"âš ï¸ \") + \" BM25 Index: \" + \n",
    "      (\"Tested index building, search, term statistics, and optimization\" if BM25_AVAILABLE else \"Skipped (rank_bm25 not available)\"))\n",
    "print(\"6. âœ… Index Builder: Tested comprehensive index building with all components\")\n",
    "print(\"7. âœ… Global Instances: Verified all global instance getters work correctly\")\n",
    "print(\"8. \" + (\"âœ…\" if 'embedder' in locals() else \"âš ï¸ \") + \" Embedding Integration: \" + \n",
    "      (\"Tested integration with embeddings module\" if 'embedder' in locals() else \"Run embedding tests first\"))\n",
    "\n",
    "print(\"\\nðŸŽ¯ NEXT STEPS:\")\n",
    "print(\"1. âœ… Vector Store Module - Testing Complete\")\n",
    "print(\"2. â³ Retrieval Module - Ready for testing\")\n",
    "print(\"3. â³ Generation Module - Ready for testing\")\n",
    "print(\"4. â³ RAGAS Module - Ready for testing\")\n",
    "print(\"5. â³ End-to-End Pipeline - Ready for final integration test\")\n",
    "\n",
    "print(\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "print(\"1. Ensure rank_bm25 is installed for full BM25 functionality\")\n",
    "print(\"2. Test with larger datasets to validate performance\")\n",
    "print(\"3. Verify persistence by restarting and reloading indexes\")\n",
    "\n",
    "print(\"\\nâœ… VECTOR STORE MODULE TESTING COMPLETE\")\n",
    "print(\"   Next step: Retrieval Module Testing\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc08326-be4b-4787-a2ab-4847c3e941b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6597a66-6fbc-4d5d-bba2-b663e2726f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9daa7-932a-41eb-af28-c5691e1d76e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7bb1b-061c-455a-8ff4-e85631c4b5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143b8b3-1c2c-4702-b3c8-3d46df401ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b5138-06c3-4eff-b130-9e2f4d3636b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19bcc045-fdb2-4821-a9d9-4e0fe6cb09ae",
   "metadata": {},
   "source": [
    "### SUMMARY REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0405448-3301-4ab9-9d53-27bf0a4b0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: INTEGRATION TEST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nâœ… PIPELINE STATUS: {'WORKING' if parsed_documents else 'NEEDS ATTENTION'}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Files Processed:\")\n",
    "for file_type in parsed_documents.keys():\n",
    "    print(f\"  âœ“ {file_type.upper()}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Statistics:\")\n",
    "print(f\"  Documents Parsed: {len(parsed_documents)}\")\n",
    "if token_stats:\n",
    "    total_tokens = sum(stats['tokens'] for stats in token_stats.values())\n",
    "    print(f\"  Total Tokens: {total_tokens:,}\")\n",
    "if 'fixed_chunks' in locals():\n",
    "    print(f\"  Chunks Created (Fixed): {len(fixed_chunks)}\")\n",
    "if 'semantic_chunks' in locals():\n",
    "    print(f\"  Chunks Created (Semantic): {len(semantic_chunks)}\")\n",
    "if 'hierarchical_chunks' in locals():\n",
    "    print(f\"  Chunks Created (Hierarchical): {len(hierarchical_chunks)}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"  1. âœ“ Configuration - Working\")\n",
    "print(\"  2. âœ“ Document Parsing - Working\")\n",
    "print(\"  3. âœ“ Text Cleaning - Working\")\n",
    "print(\"  4. âœ“ Token Counting - Working\")\n",
    "print(\"  5. âœ“ Chunking - Working\")\n",
    "print(\"  6. â³ Embedding Generation - Ready for next phase\")\n",
    "print(\"  7. â³ Vector Indexing - Ready for next phase\")\n",
    "print(\"  8. â³ Retrieval Testing - Ready for next phase\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ’¡ TIP: Update TEST_FILES paths at the top to test with your actual files!\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EMBEDDINGS MODULE TEST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get final component stats\n",
    "try:\n",
    "    model_stats = model_loader.get_model_info()\n",
    "    batch_stats = batch_processor.get_processing_stats()\n",
    "    cache_stats = cache.get_stats()\n",
    "    embedder_info = embedder.get_model_info()\n",
    "    \n",
    "    print(\"\\nðŸ“Š FINAL COMPONENT STATISTICS:\")\n",
    "    print(f\"\\nModel Loader:\")\n",
    "    print(f\"  Model: {model_stats.get('model_name', 'unknown')}\")\n",
    "    print(f\"  Loaded: {model_stats.get('loaded', False)}\")\n",
    "    print(f\"  Cache size: {model_stats.get('cache_size', 0)}\")\n",
    "    \n",
    "    print(f\"\\nBatch Processor:\")\n",
    "    print(f\"  Total batches: {batch_stats.get('total_batches', 0)}\")\n",
    "    print(f\"  Total texts: {batch_stats.get('total_texts', 0)}\")\n",
    "    print(f\"  Failed batches: {batch_stats.get('failed_batches', 0)}\")\n",
    "    print(f\"  Success rate: {batch_stats.get('success_rate', 0):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nEmbedder:\")\n",
    "    print(f\"  Model: {embedder_info.get('model_name', 'unknown')}\")\n",
    "    print(f\"  Dimension: {embedder_info.get('embedding_dim', 'unknown')}\")\n",
    "    print(f\"  Device: {embedder_info.get('device', 'unknown')}\")\n",
    "    \n",
    "    print(f\"\\nEmbedding Cache:\")\n",
    "    print(f\"  Cache size: {cache_stats.get('cache_size', 0)}\")\n",
    "    print(f\"  Max size: {cache_stats.get('max_size', 0)}\")\n",
    "    print(f\"  Hits: {cache_stats.get('hits', 0)}\")\n",
    "    print(f\"  Misses: {cache_stats.get('misses', 0)}\")\n",
    "    print(f\"  Hit rate: {cache_stats.get('hit_rate_percentage', 0):.1f}%\")\n",
    "    print(f\"  Embeddings generated: {cache_stats.get('embeddings_generated', 0)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting final stats: {e}\")\n",
    "\n",
    "print(\"\\nâœ… EMBEDDINGS MODULE TESTING COMPLETE\")\n",
    "print(\"   Next step: Vector Store Module Testing\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62f024-d2dc-46c9-9b4d-6ab096950932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
