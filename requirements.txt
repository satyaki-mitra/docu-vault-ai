# Core Dependencies
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
python-multipart==0.0.6

# AI/ML & NLP
ollama==0.1.7
sentence-transformers==2.2.2
transformers==4.35.2
torch==2.1.2               # Must match sentence-transformers 2.2.2
tokenizers==0.15.0
accelerate==0.24.1

# Vector Database & Search
faiss-cpu==1.7.4           # Compatible with torch 2.1.x + numpy 1.24.x

# Document Processing
pymupdf
pypdf2==3.0.1
python-docx==1.1.0
python-pptx==0.6.21

# OCR & Image Processing
paddleocr
easyocr
paddlepaddle
pillow
opencv-python

# Archive Handling
py7zr==0.20.5
rarfile==4.1
python-magic==0.4.27

# Web Scraping (for future - optional)
playwright==1.40.0
beautifulsoup4==4.12.2
requests==2.31.0
aiohttp==3.9.1

# Text Processing & Utilities
chardet==5.2.0
numpy==1.24.3              # Critical: faiss-cpu 1.7.4 requires numpy <1.25
nltk==3.8.1
tqdm==4.66.1
filetype==1.2.0            # From your utils/file_handler.py

# Caching & Performance
redis==5.0.1
psutil==5.9.6

# ============================================================================
# EVALUATION MODULE DEPENDENCIES (NEW/ENHANCED)
# ============================================================================

# Ragas Evaluation Framework
ragas==0.0.22              # Core evaluation framework
datasets==2.14.6           # Required by Ragas for dataset handling
evaluate==0.4.1            # Hugging Face evaluation framework
rouge-score==0.1.2         # For ROUGE metrics in evaluation
bert-score==0.3.13         # For BERTScore in evaluation

# LangSmith Integration
langsmith                  # Updated to latest version for better features
langchain                  # Required by LangSmith
langchain-community

# GPT-2 Evaluation Judge
transformers[torch]==4.35.2 # Already included, ensures torch compatibility

# Evaluation Metrics & Utilities
scikit-learn==1.3.2        # For additional metrics (TF-IDF, cosine similarity)
scipy==1.11.4              # Required by scikit-learn and other scientific computing
pandas==2.1.4              # For dataset handling and analysis
seaborn==0.13.0            # For evaluation visualization (optional)
matplotlib==3.8.2          # For plotting evaluation results (optional)

# Development & Testing
pytest==7.4.3
pytest-asyncio==0.21.1
black==23.11.0
flake8==6.1.0
mypy==1.7.1

# Async & Concurrency
aiofiles==23.2.1
asyncio-throttle==1.0.2

# System & Utilities
python-dateutil==2.8.2
pathlib2==2.3.7.post1
typing-extensions==4.8.0

# LLM & Embedding Extras
rank-bm25==0.2.2           # From your retrieval/bm25_index.py
sentencepiece==0.1.99      # Required by some BGE models
protobuf<=3.20.3           # Avoids PaddlePaddle conflict

# Logging & Structured Output
coloredlogs==15.0.1        # Optional but nice for console

# ============================================================================
# OPTIONAL EVALUATION EXTRAS (Comment out if not needed)
# ============================================================================

# # Advanced Evaluation (Uncomment if needed)
# # For more sophisticated evaluation metrics
# nlg-eval==2.3.0           # Additional NLP evaluation metrics
# # pycocoevalcap==1.0       # COCO evaluation metrics (for captioning)
# 
# # For evaluation visualization dashboard
# # streamlit==1.28.0        # For creating evaluation dashboards
# # plotly==5.17.0           # Interactive evaluation charts